{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RNNs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# Imports theano and Numpy\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "# Sets random number generators\n",
    "np.random.seed(42)\n",
    "rng = np.random.RandomState(1337)\n",
    "dtype = theano.config.floatX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights(n_in, n_out):\n",
    "    # The magnitude of the weights\n",
    "    mag = 4. * np.sqrt(6. / (n_in + n_out))\n",
    "    # Generates a randomized numpy array for the weights\n",
    "    W_value = np.asarray(rng.uniform(low=-mag, high=mag, size=(n_in, n_out)), dtype=dtype)\n",
    "    # converts the randomized array into a theano variable, which is more optimized and uses the gpu\n",
    "    W = theano.shared(value=W_value, name='W_{}_{}'.format(n_in, n_out), borrow=True, strict=False)\n",
    "    return W\n",
    "# It is recommended that you initialize biases at zero\n",
    "def get_bias(n_out):\n",
    "    # creates a numpy array\n",
    "    b_value = np.asarray(np.zeros((n_out,), dtype=dtype), dtype=theano.config.floatX)\n",
    "    # converts the numpy array into \n",
    "    b = theano.shared(value=b_value, name='b_{}'.format(n_out), borrow=True, strict=False)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(cost, params, learning_rate):\n",
    "    # updates each parameter based on it's influence on the cost, determined by the gradient\n",
    "    # (or derivative) of the cost with respect to each parameter\n",
    "    updates = [(p, p-learning_rate * T.grad(cost, p)) for p in params]\n",
    "    return updates\n",
    "\n",
    "def rmsprop(cost, params, learning_rate, rho=0.9, epsilon=1e-6):\n",
    "    updates = list()\n",
    "    for param in params:\n",
    "        accu = theano.shared(np.zeros(param.get_value(borrow=True).shape, dtype=dtype),\n",
    "                             broadcastable=param.broadcastable)\n",
    "        grad = T.grad(cost, param)\n",
    "        accu_new = rho * accu + (1 - rho) * grad ** 2\n",
    "\n",
    "        updates.append((accu, accu_new))\n",
    "        updates.append((param, param - (learning_rate * grad / T.sqrt(accu_new + epsilon))))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(n_in, n_out, X, output, params):\n",
    "    output = output[-1, :] # get the last timestep from the network\n",
    "    y = T.matrix(name='y', dtype=dtype) # the target variable\n",
    "    lr = T.scalar(name='lr', dtype=dtype) # the learning rate (as a variable we can change)\n",
    "\n",
    "    # minimize binary crossentropy\n",
    "    xent = -y * T.log(output) - (1 - y) * T.log(1 - output)\n",
    "    cost = xent.mean()\n",
    "    \n",
    "    # use rmsprop to get the network updates\n",
    "    # updates = sgd(cost, params, lr)\n",
    "    updates = rmsprop(cost, params, lr)\n",
    "\n",
    "    # generate our testing data\n",
    "    t_sets = 10\n",
    "    X_datas = [np.asarray(rng.rand(20, n_in) > 0.5, dtype=dtype) for _ in range(t_sets)]\n",
    "    y_datas = [np.asarray(rng.rand(1, n_out) > 0.5, dtype=dtype) for _ in range(t_sets)]\n",
    "\n",
    "    # theano functions for training and testing\n",
    "    train = theano.function([X, y, lr], [cost], updates=updates)\n",
    "    test = theano.function([X], [output])\n",
    "\n",
    "    # some starting parameters\n",
    "    l = 0.05\n",
    "    n_train = 1000\n",
    "\n",
    "    # calculate and display the cost\n",
    "    cost = sum([train(X_data, y_data, 0)[0] for X_data, y_data in zip(X_datas, y_datas)])\n",
    "    print('Before training:', cost)\n",
    "\n",
    "    for i in range(n_train):\n",
    "        for X_data, y_data in zip(X_datas, y_datas):\n",
    "            train(X_data, y_data, l)\n",
    "\n",
    "        if (i+1) % (n_train / 5) == 0:\n",
    "            cost = sum([train(X_data, y_data, 0)[0] for X_data, y_data in zip(X_datas, y_datas)])\n",
    "            print('%d (lr = %f):' % (i+1, l), cost)\n",
    "            l *= 0.5 # decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 37.5532688082\n",
      "200 (lr = 0.050000): 6.987509572\n",
      "400 (lr = 0.025000): 0.158165922926\n",
      "600 (lr = 0.012500): 0.000100941914509\n",
      "800 (lr = 0.006250): 6.10472981451e-05\n",
      "1000 (lr = 0.003125): 5.16863635097e-05\n"
     ]
    }
   ],
   "source": [
    "def generate_and_test_vanilla_rnn(n_in, n_hidden, n_out):\n",
    "    # a place holder for the input matrix where each row is a data sample\n",
    "    X = T.matrix(name='X', dtype=dtype)\n",
    "\n",
    "    # the weights being used in the network\n",
    "    w_in = get_weights(n_in, n_hidden)\n",
    "    w_hidden = get_weights(n_hidden, n_hidden)\n",
    "    w_out = get_weights(n_hidden, n_out)\n",
    "\n",
    "    # the biases\n",
    "    b_hidden = get_bias(n_hidden)\n",
    "    b_out = get_bias(n_out)\n",
    "    h_0 = get_bias(n_hidden)\n",
    "    \n",
    "    # collect all the parameters here so we can pass them to the optimizer\n",
    "    params = [w_in, b_hidden, w_out, b_out, w_hidden, h_0]\n",
    "    \n",
    "    # define the recurrence here\n",
    "    def step(x_t, h_tm1):\n",
    "        h_t = T.tanh(T.dot(x_t, w_in) + T.dot(h_tm1, w_hidden) + b_hidden)\n",
    "        y_t = T.nnet.sigmoid(T.dot(h_t, w_out) + b_out)\n",
    "        return h_t, y_t\n",
    "\n",
    "    [_, output], _ = theano.scan(fn=step, sequences=X, outputs_info=[h_0, None], n_steps=X.shape[0])\n",
    "\n",
    "    test(n_in, n_out, X, output, params)\n",
    "\n",
    "generate_and_test_vanilla_rnn(10, 50, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Basic RNN\n",
    "![Basic RNN](https://github.com/codekansas/pydata-carolinas-2016/blob/master/images/basic_rnn_graphic.png?raw=true)\n",
    "\n",
    "### LSTM\n",
    "![LSTM](https://github.com/codekansas/pydata-carolinas-2016/blob/master/images/lstm_graphic.png?raw=true)\n",
    "\n",
    "\n",
    "The equations defining an LSTM are as follow:\n",
    "\n",
    "```\n",
    "input_gate = tanh(dot(input_vector, W_input) + dot(prev_hidden, U_input) + b_input)\n",
    "forget_gate = tanh(dot(input_vector, W_forget) + dot(prev_hidden, U_forget) + b_forget)\n",
    "output_gate = tanh(dot(input_vector, W_output) + dot(prev_hidden, U_output) + b_output)\n",
    "\n",
    "candidate_state = tanh(dot(x, W_hidden) + dot(prev_hidden, U_hidden) + b_hidden)\n",
    "memory_unit = prev_candidate_state * forget_gate + candidate_state * input_gate\n",
    "\n",
    "new_hidden_state = tanh(memory_unit) * output_gate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 9.11031445437\n",
      "200 (lr = 0.050000): 1.93198257786e-05\n",
      "400 (lr = 0.025000): 1.24381536375e-05\n",
      "600 (lr = 0.012500): 1.05390832752e-05\n",
      "800 (lr = 0.006250): 9.78851367624e-06\n",
      "1000 (lr = 0.003125): 9.45131889706e-06\n"
     ]
    }
   ],
   "source": [
    "def generate_and_test_lstm(n_in, n_hidden, n_out):\n",
    "    X = T.matrix(name='X', dtype=dtype)\n",
    "\n",
    "    # there are a lot of parameters, so let's add them incrementally\n",
    "    params = list()\n",
    "\n",
    "    # input gate\n",
    "    w_in_input = get_weights(n_in, n_hidden)\n",
    "    w_hidden_input = get_weights(n_hidden, n_hidden)\n",
    "    b_input = get_bias(n_hidden)\n",
    "    params += [w_in_input, w_hidden_input, b_input]\n",
    "\n",
    "    # forget gate\n",
    "    w_in_forget = get_weights(n_in, n_hidden)\n",
    "    w_hidden_forget = get_weights(n_hidden, n_hidden)\n",
    "    b_forget = get_bias(n_hidden)\n",
    "    params += [w_in_forget, w_hidden_forget, b_forget]\n",
    "\n",
    "    # output gate\n",
    "    w_in_output = get_weights(n_in, n_hidden)\n",
    "    w_hidden_output = get_weights(n_hidden, n_hidden)\n",
    "    b_output = get_bias(n_hidden)\n",
    "    params += [w_in_output, w_hidden_output, b_output]\n",
    "\n",
    "    # hidden state\n",
    "    w_in_hidden = get_weights(n_in, n_hidden)\n",
    "    w_hidden_hidden = get_weights(n_hidden, n_hidden)\n",
    "    b_hidden = get_bias(n_hidden)\n",
    "    params += [w_in_hidden, w_hidden_hidden, b_hidden]\n",
    "\n",
    "    # output\n",
    "    w_out = get_weights(n_hidden, n_out)\n",
    "    b_out = get_bias(n_out)\n",
    "    params += [w_out, b_out]\n",
    "\n",
    "    # starting hidden and memory unit state\n",
    "    h_0 = get_bias(n_hidden)\n",
    "    c_0 = get_bias(n_hidden)\n",
    "    params += [h_0, c_0]\n",
    "    \n",
    "    # define the recurrence here\n",
    "    def step(x_t, h_tm1, c_tm1):\n",
    "        input_gate = T.nnet.sigmoid(T.dot(x_t, w_in_input) + T.dot(h_tm1, w_hidden_input) + b_input)\n",
    "        forget_gate = T.nnet.sigmoid(T.dot(x_t, w_in_forget) + T.dot(h_tm1, w_hidden_forget) + b_forget)\n",
    "        output_gate = T.nnet.sigmoid(T.dot(x_t, w_in_output) + T.dot(h_tm1, w_hidden_output) + b_output)\n",
    "\n",
    "        candidate_state = T.tanh(T.dot(x_t, w_in_hidden) + T.dot(h_tm1, w_hidden_hidden) + b_hidden)\n",
    "        memory_unit = c_tm1 * forget_gate + candidate_state * input_gate\n",
    "\n",
    "        h_t = T.tanh(memory_unit) * output_gate\n",
    "        y_t = T.nnet.sigmoid(T.dot(h_t, w_out) + b_out)\n",
    "\n",
    "        return h_t, memory_unit, y_t\n",
    "\n",
    "    [_, _, output], _ = theano.scan(fn=step, sequences=X, outputs_info=[h_0, c_0, None], n_steps=X.shape[0])\n",
    "\n",
    "    test(n_in, n_out, X, output, params)\n",
    "\n",
    "generate_and_test_lstm(10, 50, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRUs, for good measure\n",
    "\n",
    "Since we've done LSTMs, we should do GRUs as well. Again from Christopher Olah's blog post, the GRU looks like:\n",
    "\n",
    "![GRU](https://github.com/codekansas/pydata-carolinas-2016/blob/master/images/gru_graphic.png?raw=true)\n",
    "\n",
    "The equations are simpler than the LSTM equations, and look like:\n",
    "\n",
    "```\n",
    "update_gate = tanh(dot(input_vector, W_update) + dot(prev_hidden, U_update) + b_update)\n",
    "reset_gate = tanh(dot(input_vector, W_reset) + dot(prev_hidden, U_reset) + b_reset)\n",
    "\n",
    "reset_hidden = prev_hidden * reset_gate\n",
    "temp_state = tanh(dot(input_vector, W_hidden) + dot(reset_hidden, U_reset) + b_hidden)\n",
    "new_hidden_state = (1 - update_gate) * temp_state + update_gate * prev_hidden\n",
    "```\n",
    "\n",
    "Let's go ahead and implement this in Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 10.9584113373\n",
      "200 (lr = 0.050000): 1.03100185903e-05\n",
      "400 (lr = 0.025000): 6.60928641391e-06\n",
      "600 (lr = 0.012500): 5.6003682784e-06\n",
      "800 (lr = 0.006250): 5.20264902116e-06\n",
      "1000 (lr = 0.003125): 5.02412467895e-06\n"
     ]
    }
   ],
   "source": [
    "def generate_and_test_gru(n_in, n_hidden, n_out):\n",
    "    X = T.matrix(name='X', dtype=dtype)\n",
    "\n",
    "    # there are a lot of parameters, so let's add them incrementally\n",
    "    params = list()\n",
    "\n",
    "    # update gate\n",
    "    w_in_update = get_weights(n_in, n_hidden)\n",
    "    w_hidden_update = get_weights(n_hidden, n_hidden)\n",
    "    b_update = get_bias(n_hidden)\n",
    "    params += [w_in_update, w_hidden_update, b_update]\n",
    "\n",
    "    # reset gate\n",
    "    w_in_reset = get_weights(n_in, n_hidden)\n",
    "    w_hidden_reset = get_weights(n_hidden, n_hidden)\n",
    "    b_reset = get_bias(n_hidden)\n",
    "    params += [w_in_reset, w_hidden_reset, b_reset]\n",
    "\n",
    "    # hidden layer\n",
    "    w_in_hidden = get_weights(n_in, n_hidden)\n",
    "    w_reset_hidden = get_weights(n_hidden, n_hidden)\n",
    "    b_in_hidden = get_bias(n_hidden)\n",
    "    params += [w_in_hidden, w_reset_hidden, b_in_hidden]\n",
    "\n",
    "    # output\n",
    "    w_out = get_weights(n_hidden, n_out)\n",
    "    b_out = get_bias(n_out)\n",
    "    params += [w_out, b_out]\n",
    "\n",
    "    # starting hidden state\n",
    "    h_0 = get_bias(n_hidden)\n",
    "    params += [h_0]\n",
    "    \n",
    "    # define the recurrence here\n",
    "    def step(x_t, h_tm1):\n",
    "        update_gate = T.nnet.sigmoid(T.dot(x_t, w_in_update) + T.dot(h_tm1, w_hidden_update) + b_update)\n",
    "        reset_gate = T.nnet.sigmoid(T.dot(x_t, w_in_reset) + T.dot(h_tm1, w_hidden_reset) + b_reset)\n",
    "        h_t_temp = T.tanh(T.dot(x_t, w_in_hidden) + T.dot(h_tm1 * reset_gate, w_reset_hidden) + b_in_hidden)\n",
    "\n",
    "        h_t = (1 - update_gate) * h_t_temp + update_gate * h_tm1\n",
    "        y_t = T.nnet.sigmoid(T.dot(h_t, w_out) + b_out)\n",
    "\n",
    "        return h_t, y_t\n",
    "\n",
    "    [_, output], _ = theano.scan(fn=step, sequences=X, outputs_info=[h_0, None], n_steps=X.shape[0])\n",
    "\n",
    "    test(n_in, n_out, X, output, params)\n",
    "\n",
    "generate_and_test_gru(10, 50, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making This A Lot Less Painful with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TomCat/python-neural-nets/venv/lib/python2.7/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"si..., inputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- SimpleRNN --\n",
      "Error before: 1.64460468292\n",
      "Error after: 0.805904746056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TomCat/python-neural-nets/venv/lib/python2.7/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ls..., inputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- LSTM --\n",
      "Error before: 0.797749400139\n",
      "Error after: 1.08644201191e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TomCat/python-neural-nets/venv/lib/python2.7/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"gr..., inputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- GRU --\n",
      "Error before: 6.65989971161\n",
      "Error after: 1.08644201191e-07\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "input_dims, output_dims = 10, 2\n",
    "sequence_length = 20\n",
    "n_test = 10\n",
    "\n",
    "# generate some random data to train on\n",
    "X_data = np.asarray([np.asarray(rng.rand(20, input_dims) > 0.5, dtype=dtype) for _ in range(n_test)])\n",
    "y_data = np.asarray([np.asarray(rng.rand(output_dims,) > 0.5, dtype=dtype) for _ in range(n_test)])\n",
    "\n",
    "# put together rnn models\n",
    "from keras.layers import Input\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "\n",
    "input_sequence = Input(shape=(sequence_length, input_dims,), dtype='float32')\n",
    "\n",
    "# this is so much easier!\n",
    "vanilla = SimpleRNN(output_dims, return_sequences=False)\n",
    "lstm = LSTM(output_dims, return_sequences=False)\n",
    "gru = GRU(output_dims, return_sequences=False)\n",
    "rnns = [vanilla, lstm, gru]\n",
    "\n",
    "# train the models\n",
    "for rnn in rnns:\n",
    "    model = Model(input=[input_sequence], output=rnn(input_sequence))\n",
    "    model.compile(optimizer=RMSprop(lr=0.1), loss='binary_crossentropy')\n",
    "    print('-- %s --' % rnn.__class__.__name__)\n",
    "    print('Error before: {}'.format(model.evaluate([X_data], [y_data], verbose=0)))\n",
    "    model.fit([X_data], [y_data], epochs=1000, verbose=0)\n",
    "    print('Error after: {}'.format(model.evaluate([X_data], [y_data], verbose=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
