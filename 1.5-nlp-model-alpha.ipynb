{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handles Importing all the necessary libraries\n",
    "import itertools\n",
    "import h5py\n",
    "import numpy as np\n",
    "import string\n",
    "import keras\n",
    "from keras.layers import Input, Embedding, merge, Flatten, Reshape, Lambda, LSTM, Dropout\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 15, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepares the Data Set to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes Removed: 323\n",
      "Number of Sentences: 7441\n",
      "Vocabulary: 6463\n"
     ]
    }
   ],
   "source": [
    "#imports a massive dataset of sentences found on wikipedia\n",
    "with open('ignored_assets/lang-train-data-angelas-ashes.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line,\n",
    "# also strips the punctuation\n",
    "sentences = [x.strip().translate(None, string.punctuation) for x in content] \n",
    "\n",
    "lemma = lambda x: x.strip().lower().split(' ')\n",
    "sentences_lemmatized = [lemma(sentence) for sentence in sentences]\n",
    "words = set(itertools.chain(*sentences_lemmatized))\n",
    "# set(['boy', 'fed', 'ate', 'cat', 'kicked', 'hat'])\n",
    "\n",
    "# dictionaries for converting words to integers and vice versa\n",
    "word2idx = dict((v, i) for i, v in enumerate(words))\n",
    "idx2word = list(words)\n",
    "\n",
    "# convert the sentences a numpy array\n",
    "to_idx = lambda x: [word2idx[word] for word in x]\n",
    "sentences_idx = [to_idx(sentence) for sentence in sentences_lemmatized]\n",
    "# Sets the maximum word length of each sentence\n",
    "max_len = 40\n",
    "# a list of all the indices I remove that are longer than max_len\n",
    "indices_removed = []\n",
    "# If the sentence is too long, good by\n",
    "for i in range(len(sentences_idx)-1, -1, -1):\n",
    "    if len(sentences_idx[i]) > max_len:\n",
    "        indices_removed = indices_removed + [i]\n",
    "        sentences_idx.pop(i)\n",
    "    elif len(sentences_idx[i]) < max_len:\n",
    "        sentences_idx[i] = sentences_idx[i] + [0] * (max_len - len(sentences_idx[i]));\n",
    "\n",
    "print \"Indexes Removed: {}\".format(len(indices_removed))\n",
    "print \"Number of Sentences: {}\".format(len(sentences_idx))\n",
    "print \"Vocabulary: {}\".format(len(word2idx))\n",
    "sentences_array = np.asarray(sentences_idx, dtype='int32')\n",
    "\n",
    "# Prepares the datasets as input and output\n",
    "dataX = np.array(sentences_array[:len(sentences_array)-1]).copy()\n",
    "dataY = np.array(sentences_array[1:]).copy()\n",
    "\n",
    "# scales the output\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "dataY = scaler.fit_transform(dataY)\n",
    "# reshapes the dataY\n",
    "new_dataY = dataY.reshape((1,) + dataY.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create and Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Epoch 2/2\n",
      ": [  4.00357693e-02  -1.41882775e-02  -1.69515628e-02   2.72160443e-03\n",
      "  -5.19732479e-03  -1.00261415e-03  -1.50458759e-03  -8.53088452e-04\n",
      "   3.68796661e-02  -1.40176332e-02   5.93608897e-03  -9.12672877e-02\n",
      "   5.74352918e-04  -1.37870992e-03  -3.51639086e-04  -3.04035237e-03\n",
      "  -2.64175492e-03   7.65870034e-04  -6.68601692e-03   7.05517596e-03\n",
      "  -1.28488708e-02   7.99107831e-04  -3.54190677e-04   2.08475478e-02\n",
      "  -8.44246184e-04  -6.08783308e-03   6.53023235e-05  -7.27649487e-04\n",
      "  -1.68303653e-04   2.02140328e-03   5.04109485e-04  -3.82608129e-03\n",
      "   3.13007794e-02  -5.59197040e-03  -2.94063194e-03   1.26788300e-03\n",
      "   6.46054521e-02   5.48614655e-03  -6.92775520e-03  -4.59323497e-03\n",
      "  -6.52802782e-03  -2.57798261e-03  -5.29891849e-02   2.96514574e-02\n",
      "   4.69198544e-03  -1.22453077e-02  -8.46719649e-03  -1.40683842e-03\n",
      "  -2.48018210e-03   4.01063962e-03  -3.40918475e-03  -1.44209759e-03\n",
      "   2.53453050e-02  -4.23171977e-03   3.36540528e-02   8.69504921e-03\n",
      "   1.43657010e-02   6.51292596e-03  -5.78218140e-04   7.43844965e-03\n",
      "   1.86012313e-02   8.93426710e-04   6.36727177e-03  -2.88408221e-04\n",
      "   4.88135442e-02  -1.02783076e-03   7.64559372e-04  -3.72628914e-04\n",
      "  -2.43673963e-03  -2.79840454e-03   8.91048554e-03  -1.01583684e-02\n",
      "   2.56965659e-03   7.65495673e-02  -7.87566695e-03   1.34566624e-03\n",
      "  -8.71483516e-03  -1.95400361e-02   3.25511731e-02   1.76770356e-03\n",
      "  -1.58356763e-02   1.16544906e-02  -8.59140884e-03  -2.92923898e-02\n",
      "  -2.29119901e-02   4.71119024e-03   4.23507057e-02   4.24357783e-03\n",
      "   2.07276326e-02  -9.97570530e-03  -1.08251628e-02   1.33323995e-03\n",
      "  -1.01117611e-01  -1.09701056e-03  -3.58388817e-04  -9.54672636e-04\n",
      "   1.75628241e-03   3.03234551e-02  -1.18257396e-03  -4.65566758e-03\n",
      "   1.71415582e-02  -1.42423352e-02  -8.89713876e-04   4.50018700e-03\n",
      "  -5.72749123e-04   2.74787284e-02  -4.30250913e-03  -3.85896442e-03\n",
      "   7.37750495e-04  -7.57473484e-02  -1.68559775e-02   3.72360498e-02\n",
      "  -4.11437787e-02   4.40812111e-03  -9.65023227e-03  -3.12232040e-02\n",
      "  -1.11133177e-02   3.44208837e-03  -6.44185022e-03  -5.69226965e-03\n",
      "  -8.12336244e-03  -4.41840570e-03  -4.96257655e-03  -6.52215956e-03\n",
      "  -4.33441345e-03  -3.58654596e-02  -1.67758428e-02   2.46339105e-03\n",
      "  -9.39358212e-03  -5.21095796e-03   1.18675120e-02   4.34941670e-04\n",
      "  -3.86913819e-03   3.11673735e-03   3.60609079e-03   1.04671223e-02\n",
      "   9.11243726e-04  -1.00177182e-02   7.39683677e-03   1.51931978e-04\n",
      "   1.15365989e-03   4.48275497e-03  -2.05331445e-02   1.13827316e-03\n",
      "   1.04580889e-03  -4.69285063e-02  -1.06642349e-02  -1.57304015e-03\n",
      "  -1.18836542e-05   3.15812789e-03]\n",
      "raining: [ 0.03640177  0.12818265  0.01372289 -0.07351249 -0.02374011  0.09487683\n",
      "  0.0433732   0.04404478 -0.01776041  0.06745459 -0.00286411  0.01895979\n",
      " -0.05484595  0.05864463 -0.12929837 -0.01469977  0.00402701  0.00675718\n",
      "  0.0215612  -0.02186095 -0.01381361 -0.0107404   0.05269127  0.07911718\n",
      " -0.00290054 -0.05537338 -0.06785364 -0.08506193  0.01177081 -0.02649437\n",
      " -0.05064993 -0.05145376 -0.0674793   0.07548983  0.01296352  0.09791539\n",
      "  0.08552454 -0.02356911 -0.07237744 -0.01096903 -0.006885   -0.03827354\n",
      " -0.03174068 -0.00787063 -0.04925338  0.00229629  0.0077678  -0.08326267\n",
      " -0.00551886  0.09195291  0.04402266 -0.08622423  0.01513416 -0.03815728\n",
      "  0.02266482 -0.00476463 -0.05208175  0.08162976  0.00092355  0.05464465\n",
      " -0.02888134 -0.05402563  0.01103396 -0.03199965  0.00522128 -0.02198495\n",
      "  0.06192987 -0.0204823  -0.10902791 -0.05901102 -0.01351842  0.01333721\n",
      " -0.05639989 -0.11433565  0.05190429 -0.03017716 -0.0230935   0.06175212\n",
      " -0.04339445  0.09395629  0.01180008  0.10922419 -0.08501718 -0.06781075\n",
      "  0.05256647  0.04886508 -0.07259365  0.05191545 -0.00075247  0.04554192\n",
      "  0.15916488  0.02544724  0.02170086 -0.01660189 -0.06308845  0.06726742\n",
      "  0.0108175   0.0372175  -0.12469564 -0.07858919 -0.15503077  0.05667662\n",
      " -0.07538255  0.03051416  0.02042573 -0.04509759 -0.10876151 -0.0664857\n",
      "  0.01786916  0.00129456  0.0193755  -0.0226789  -0.03723834  0.07771371\n",
      " -0.05380299  0.0285024   0.10782998 -0.01666236 -0.01076502 -0.11430924\n",
      "  0.08165954  0.03215412 -0.01308965  0.00982364 -0.01509665 -0.03796924\n",
      " -0.12882629  0.01531947  0.14881955  0.03247804  0.11176392 -0.04050779\n",
      "  0.10078315 -0.08661436  0.19176143 -0.1092443   0.05043689 -0.08030772\n",
      " -0.01809113 -0.07776777  0.01098959 -0.05753703  0.0674371   0.07886758\n",
      "  0.00228664  0.12507406  0.11367672 -0.03604223  0.11414313  0.01070274]\n",
      "Evaluation of the Model\n",
      "7392/7440 [============================>.] - ETA: 0s0.0944499534785\n",
      "Saving the Weights\n"
     ]
    }
   ],
   "source": [
    "n_words = len(words)\n",
    "n_embed_dims = 150\n",
    "\n",
    "# put together a model to predict\n",
    "input_sentence = Input(shape=(max_len,), dtype='int32')\n",
    "input_embedding = Embedding(n_words, n_embed_dims)(input_sentence)\n",
    "output = SimpleRNN(max_len)(input_embedding)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.abs(y_true - y_pred))\n",
    "\n",
    "model = Model(inputs=[input_sentence], outputs=[output])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0), \n",
    "            loss='mae')\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "\n",
    "# fit the model to predict what color each person is\n",
    "model.fit(dataX, dataY, epochs=2, verbose=3)\n",
    "embeddings = model.layers[1].get_weights()\n",
    "\n",
    "# print out the embedding vector associated with each word\n",
    "for i in range(2):\n",
    "    print('{}: {}'.format(idx2word[i], embeddings[0][i]))\n",
    "    \n",
    "print \"Evaluation of the Model\"\n",
    "print model.evaluate(dataX, dataY)\n",
    "print \"Saving the Weights\"\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Training For The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "22s - loss: 0.0907\n",
      "Epoch 2/50\n",
      "22s - loss: 0.0907\n",
      "Epoch 3/50\n",
      "22s - loss: 0.0906\n",
      "Epoch 4/50\n",
      "21s - loss: 0.0906\n",
      "Epoch 5/50\n",
      "21s - loss: 0.0905\n",
      "Epoch 6/50\n",
      "22s - loss: 0.0905\n",
      "Epoch 7/50\n",
      "22s - loss: 0.0904\n",
      "Epoch 8/50\n",
      "21s - loss: 0.0904\n",
      "Epoch 9/50\n",
      "21s - loss: 0.0903\n",
      "Epoch 10/50\n",
      "22s - loss: 0.0903\n",
      "Epoch 11/50\n",
      "21s - loss: 0.0903\n",
      "Epoch 12/50\n",
      "22s - loss: 0.0902\n",
      "Epoch 13/50\n",
      "21s - loss: 0.0901\n",
      "Epoch 14/50\n",
      "21s - loss: 0.0901\n",
      "Epoch 15/50\n",
      "22s - loss: 0.0900\n",
      "Epoch 16/50\n",
      "21s - loss: 0.0900\n",
      "Epoch 17/50\n",
      "22s - loss: 0.0900\n",
      "Epoch 18/50\n",
      "22s - loss: 0.0899\n",
      "Epoch 19/50\n",
      "22s - loss: 0.0899\n",
      "Epoch 20/50\n",
      "23s - loss: 0.0898\n",
      "Epoch 21/50\n",
      "23s - loss: 0.0898\n",
      "Epoch 22/50\n",
      "22s - loss: 0.0897\n",
      "Epoch 23/50\n",
      "22s - loss: 0.0897\n",
      "Epoch 24/50\n",
      "22s - loss: 0.0897\n",
      "Epoch 25/50\n",
      "22s - loss: 0.0896\n",
      "Epoch 26/50\n",
      "22s - loss: 0.0896\n",
      "Epoch 27/50\n",
      "22s - loss: 0.0895\n",
      "Epoch 28/50\n",
      "22s - loss: 0.0895\n",
      "Epoch 29/50\n",
      "22s - loss: 0.0894\n",
      "Epoch 30/50\n",
      "22s - loss: 0.0894\n",
      "Epoch 31/50\n",
      "22s - loss: 0.0893\n",
      "Epoch 32/50\n",
      "22s - loss: 0.0893\n",
      "Epoch 33/50\n",
      "22s - loss: 0.0893\n",
      "Epoch 34/50\n",
      "22s - loss: 0.0893\n",
      "Epoch 35/50\n",
      "22s - loss: 0.0892\n",
      "Epoch 36/50\n",
      "22s - loss: 0.0892\n",
      "Epoch 37/50\n",
      "21s - loss: 0.0891\n",
      "Epoch 38/50\n",
      "21s - loss: 0.0891\n",
      "Epoch 39/50\n",
      "22s - loss: 0.0891\n",
      "Epoch 40/50\n",
      "22s - loss: 0.0890\n",
      "Epoch 41/50\n",
      "22s - loss: 0.0890\n",
      "Epoch 42/50\n",
      "22s - loss: 0.0889\n",
      "Epoch 43/50\n",
      "21s - loss: 0.0889\n",
      "Epoch 44/50\n",
      "21s - loss: 0.0889\n",
      "Epoch 45/50\n",
      "22s - loss: 0.0888\n",
      "Epoch 46/50\n",
      "22s - loss: 0.0888\n",
      "Epoch 47/50\n",
      "21s - loss: 0.0888\n",
      "Epoch 48/50\n",
      "22s - loss: 0.0887\n",
      "Epoch 49/50\n",
      "22s - loss: 0.0887\n",
      "Epoch 50/50\n",
      "21s - loss: 0.0886\n",
      "7392/7440 [============================>.] - ETA: 0s  0.0887056428258\n",
      "saving the weights\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataX, dataY, epochs=50, batch_size=16, verbose=2)\n",
    "print \"  {}\".format(model.evaluate(dataX, dataY))\n",
    "print \"saving the weights\"\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "Epoch 2/4\n",
      "Epoch 3/4\n",
      "Epoch 4/4\n",
      ": [ -7.52860755e-02  -4.54015285e-02  -4.58094329e-02   8.84827878e-03\n",
      "  -8.48692900e-04  -3.17476653e-02  -1.13650709e-02   6.56244531e-02\n",
      "   6.53840974e-03   6.65657148e-02  -2.31543127e-02  -3.21851298e-02\n",
      "  -5.29217646e-02   1.71483925e-03   1.58685390e-02   1.47633646e-02\n",
      "  -3.10055520e-02   2.67946278e-03  -4.41819243e-03   1.17825922e-02\n",
      "   3.26833390e-02   6.82228524e-03  -2.40664147e-02   1.81059614e-02\n",
      "   4.85693328e-02   4.03854847e-02   5.97351231e-02   2.43128533e-03\n",
      "  -2.56353430e-02   1.97669603e-02  -8.07724521e-03   6.48374995e-03\n",
      "  -6.76449202e-03   3.24921831e-02   8.01629853e-03   9.09802562e-04\n",
      "  -1.93910580e-02  -2.17047911e-02  -4.88672033e-02  -4.92293611e-02\n",
      "  -1.37611022e-02   4.78589945e-02   1.62587315e-02  -1.04369866e-02\n",
      "  -3.82489036e-03  -7.70846556e-04   6.44029975e-02   6.63364481e-04\n",
      "   3.35666910e-02  -5.23372591e-02   6.23806678e-02  -1.97675209e-02\n",
      "  -1.34685580e-02  -2.47143134e-02  -3.12603428e-03   3.02405693e-02\n",
      "  -5.43108303e-03  -4.86856047e-03  -6.16468256e-03  -6.56778291e-02\n",
      "   2.80749761e-02   1.68174058e-02   5.01717953e-03   3.28464657e-02\n",
      "   4.94441949e-02   5.38286706e-03  -5.35058379e-02   1.32295350e-02\n",
      "   5.36481803e-03   4.23565581e-02   4.10231277e-02   3.82129736e-02\n",
      "  -1.79277174e-02   7.41770416e-02   3.89268138e-02   6.30560368e-02\n",
      "   1.96178183e-02   2.58695558e-02   2.65420005e-02  -5.03315330e-02\n",
      "  -1.02500185e-01  -2.38070134e-02  -3.76419537e-02  -4.41217162e-02\n",
      "   3.43981870e-02   3.58078778e-02  -4.22149971e-02  -1.95316840e-02\n",
      "   6.25744089e-02   1.94089115e-02   3.71261649e-02  -6.18134066e-02\n",
      "  -1.24759059e-02  -2.72412086e-03   1.32394698e-03  -3.53833772e-02\n",
      "   3.20545696e-02  -1.27512626e-02   5.22488775e-03   7.34908879e-02\n",
      "  -7.43532702e-02  -5.41725941e-02  -5.96406125e-02  -4.04142924e-02\n",
      "  -5.90195432e-02  -5.93596809e-02  -9.64786485e-03  -4.89757955e-02\n",
      "   1.59669612e-02   5.90473264e-02  -2.10292041e-02   2.24312451e-02\n",
      "  -3.36811095e-02   1.33087020e-02   1.85255893e-02  -7.02643767e-02\n",
      "  -3.66767012e-02   4.34864201e-02  -7.00375140e-02   4.09763604e-02\n",
      "  -9.40893218e-03  -4.01273556e-03   5.62449582e-02   6.74005076e-02\n",
      "  -1.12204151e-02  -1.65602509e-02  -2.18027793e-02  -1.45977745e-02\n",
      "  -9.77925738e-05   4.88026515e-02   8.60865600e-03  -2.01022197e-02\n",
      "  -3.49305919e-03   1.82843823e-02   4.55038026e-02   2.97041107e-02\n",
      "   1.50327652e-03   2.96132974e-02   3.24154645e-02   4.52071764e-02\n",
      "   1.75824668e-03  -2.28237566e-02   2.75641773e-02  -1.31969843e-02\n",
      "  -1.74199846e-02  -3.88109609e-02  -4.10306118e-02  -4.51851543e-03\n",
      "  -1.29248491e-02   4.94936444e-02]\n",
      "raining: [  2.17662882e-02  -3.22358608e-02   3.84049192e-02  -1.48130795e-02\n",
      "  -3.91182229e-02   1.98821183e-02   7.26884883e-03   3.82590806e-04\n",
      "   4.33988962e-03   5.22758663e-02   3.38371918e-02  -1.85643565e-02\n",
      "  -5.12135774e-02   8.95453151e-03  -4.86068102e-03   2.92333923e-02\n",
      "   1.38899060e-02  -2.50524767e-02   1.46875326e-02  -1.88776404e-02\n",
      "   1.89123545e-02   3.74245308e-02  -1.69121902e-02   1.29888356e-02\n",
      "  -2.99088471e-02   4.72432673e-02   6.61857519e-03   2.40509436e-02\n",
      "  -2.48108860e-02   5.15524894e-02  -4.37095501e-02  -3.79354320e-02\n",
      "  -3.69691327e-02   2.84210201e-02   3.42914835e-02   3.90065983e-02\n",
      "  -5.20513654e-02   1.15966611e-02  -3.85929756e-02   1.01527646e-02\n",
      "  -1.03866849e-02   1.81655400e-02  -4.30043489e-02   2.68924739e-02\n",
      "  -4.16258052e-02   1.37270372e-02   3.36869285e-02  -4.07314412e-02\n",
      "  -4.08863984e-02  -2.18793787e-02  -2.93575600e-02  -4.42397520e-02\n",
      "  -6.75795600e-04   1.86134577e-02  -1.65594183e-02  -3.04894894e-02\n",
      "  -3.66011560e-02  -2.19888277e-02   1.81427728e-02   1.51306102e-02\n",
      "   4.40629758e-02  -3.66290063e-02   1.64120886e-02  -3.52665521e-02\n",
      "  -9.02011991e-04  -2.62715667e-02   2.34425012e-02  -1.82825290e-02\n",
      "   2.04689670e-02  -3.76056135e-02   3.86128090e-02  -4.91567254e-02\n",
      "   3.90621996e-03   1.20502263e-02  -8.91838595e-03   4.74537760e-02\n",
      "  -1.67961195e-02   1.09253358e-02  -3.39184166e-03  -8.19834508e-03\n",
      "  -3.14407721e-02   3.85490693e-02   3.05205770e-02   6.03682594e-03\n",
      "  -3.88378799e-02  -1.42913461e-02   3.43349129e-02   3.30614261e-02\n",
      "  -1.39089068e-02   1.76488087e-02   1.11607956e-02   3.96731421e-02\n",
      "  -5.02227107e-04  -3.56954783e-02   1.73275247e-02  -2.37809587e-03\n",
      "  -1.09385117e-03  -1.53382681e-03  -4.42829318e-02   5.17721511e-02\n",
      "  -8.31692014e-04   3.75299603e-02  -1.41658532e-02  -5.56084886e-02\n",
      "   3.05505767e-02  -3.90999690e-02  -9.46693402e-03  -3.58318165e-02\n",
      "   1.78119820e-02  -2.10042228e-03   2.00573895e-02  -3.01384013e-02\n",
      "  -5.02838604e-02  -3.62950400e-03  -3.50329140e-03  -3.84560376e-02\n",
      "   1.53800258e-02   4.02755104e-05   2.92134397e-02  -2.90230345e-02\n",
      "  -1.85889960e-03  -3.48939188e-02   3.96542735e-02  -1.00900168e-02\n",
      "   2.05992046e-03  -4.42526415e-02  -1.80659816e-02  -3.14899907e-02\n",
      "   3.56562808e-02   4.08458933e-02   1.63240619e-02   7.38642737e-03\n",
      "   2.14235988e-02   2.74679400e-02   1.06987488e-02   3.83791467e-03\n",
      "  -3.07810921e-02   2.81800386e-02  -1.72081366e-02  -2.60625910e-02\n",
      "   2.21095681e-02  -1.99711565e-02   2.62284791e-03   1.88709609e-02\n",
      "   3.98966074e-02  -4.55852263e-02   1.96404513e-02   8.03712383e-03\n",
      "   3.77115272e-02  -9.66952369e-03]\n",
      "7360/7440 [============================>.] - ETA: 0sEvaluation: -8155.56841923\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataX, dataY, epochs=4, batch_size=64, verbose=3)\n",
    "embeddings = model.layers[1].get_weights()\n",
    "\n",
    "# print out the embedding vector associated with each word\n",
    "for i in range(2):\n",
    "    print('{}: {}'.format(idx2word[i], embeddings[0][i]))\n",
    "\n",
    "print 'Evaluation: {}'.format(model.evaluate(dataX, dataY))\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt to test the model with some custom inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.        ,  0.        , -0.        ,  0.        ,  0.00316837,\n",
       "        -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "         0.08428954,  0.28899291,  0.        ,  0.        ,  0.        ,\n",
       "        -0.31553316,  0.03101788,  0.        ,  0.        , -0.        ,\n",
       "        -0.        ,  0.        ,  0.13992527, -0.11859532,  0.        ,\n",
       "         0.        ,  0.        ,  0.01655224, -0.        ,  0.03247942,\n",
       "        -0.        ,  0.01039564, -0.        ,  0.        ,  0.19620955,\n",
       "         0.        ,  0.        ,  0.18380126,  0.12647456, -0.        ,\n",
       "         0.        ,  0.29543972,  0.59864187,  0.29531297,  0.28910464,\n",
       "        -0.        ,  0.65704238,  0.83109629,  0.65468234,  0.56219828,\n",
       "        -0.        ,  0.46520701,  0.54404628,  1.        ,  0.70375729,\n",
       "         0.84097189,  1.        ,  1.        ,  1.        , -0.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        , -0.        ,  1.        , -0.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        -0.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = [0] * (max_len - 3) + [word2idx['i'], word2idx['love'], word2idx['you']]\n",
    "model.predict(np.array(inp).reshape(1, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the Model\n",
    "This is how you export the model into a json file in order to be imported later. Then you export the model's weights. Later on in other experiments, you could effectively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Model\n",
    "This is how you import the model from a json file and the weights so that you don't need to train it every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
