{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handles Importing all the necessary libraries\n",
    "import itertools\n",
    "import h5py\n",
    "import numpy as np\n",
    "import string\n",
    "import keras\n",
    "from keras.layers import Input, Embedding, merge, Flatten, Reshape, Lambda, LSTM, Dropout, Dense\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 15, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepares the Data Set to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes Removed: 0\n",
      "Number of Sentences: 121\n",
      "Vocabulary: 142\n"
     ]
    }
   ],
   "source": [
    "convos = []\n",
    "# imports datasets\n",
    "with open('ignored_assets/lang-train-data-simple.txt') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "# takes an array of sentences and splits it into groupings of sentences\n",
    "def split_into_convo(arr, elem):\n",
    "    array = arr[:]\n",
    "    indiv_convos = []\n",
    "    for i in range(len(arr)-1, -1, -1):\n",
    "        if(array[i][0] == elem):\n",
    "            indiv_convos += [array[i+1:]]\n",
    "            array = array[:i]\n",
    "    indiv_convos += [array]\n",
    "    return indiv_convos\n",
    "\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line,\n",
    "# also strips the punctuation\n",
    "sentences = [x.strip().translate(None, string.punctuation) for x in content] \n",
    "\n",
    "lemma = lambda x: x.strip().lower().split(' ')\n",
    "sentences_lemmatized = [lemma(sentence) for sentence in sentences]\n",
    "words = set(itertools.chain(*sentences_lemmatized))\n",
    "# set(['boy', 'fed', 'ate', 'cat', 'kicked', 'hat'])\n",
    "\n",
    "# dictionaries for converting words to integers and vice versa\n",
    "word2idx = dict((v, i) for i, v in enumerate(words))\n",
    "idx2word = list(words)\n",
    "\n",
    "# convert the sentences a numpy array\n",
    "to_idx = lambda x: [word2idx[word] for word in x]\n",
    "sentences_idx = [to_idx(sentence) for sentence in sentences_lemmatized]\n",
    "# Sets the maximum word length of each sentence\n",
    "max_len = 9\n",
    "# a list of all the indices I remove that are longer than max_len\n",
    "indices_removed = []\n",
    "# If the sentence is too long, good by\n",
    "for i in range(len(sentences_idx)-1, -1, -1):\n",
    "    if len(sentences_idx[i]) > max_len:\n",
    "        indices_removed = indices_removed + [i]\n",
    "        sentences_idx.pop(i)\n",
    "    elif len(sentences_idx[i]) < max_len:\n",
    "        sentences_idx[i] = sentences_idx[i] + [0] * (max_len - len(sentences_idx[i]));\n",
    "\n",
    "print \"Indexes Removed: {}\".format(len(indices_removed))\n",
    "print \"Number of Sentences: {}\".format(len(sentences_idx))\n",
    "print \"Vocabulary: {}\".format(len(word2idx))\n",
    "sentences_array = sentences_idx#, dtype='int32'\n",
    "\n",
    "# splits the array into an array of grouped sentences (conversations)\n",
    "conversation_array = split_into_convo(sentences_array, word2idx[''])\n",
    "\n",
    "# Prepares the datasets as input and output\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(len(conversation_array)):\n",
    "    dataX += conversation_array[i][:len(conversation_array[i])-1]\n",
    "    dataY += conversation_array[i][1:]\n",
    "dataX = np.array(dataX).copy().astype('int32')\n",
    "dataY = np.array(dataY).copy().astype('int32')\n",
    "\n",
    "# scales the output\n",
    "scaler = MinMaxScaler(feature_range=(-6, 6))\n",
    "dataY = scaler.fit_transform(dataY)\n",
    "# reshapes the dataY\n",
    "new_dataY = dataY.reshape((1,) + dataY.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create and Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Epoch 2/2\n",
      ": [-0.04496781  0.02772553  0.03370747  0.04692284 -0.00934393]\n",
      "iâ€™m: [ 0.0102813  -0.01555228 -0.00485859 -0.00076571  0.0036515 ]\n",
      "Evaluation of the Model\n",
      "32/78 [===========>..................] - ETA: 0s38.9856856909\n"
     ]
    }
   ],
   "source": [
    "n_words = len(words)\n",
    "n_embed_dims = 5\n",
    "\n",
    "# put together a model to predict\n",
    "input_sentence = Input(shape=(max_len,), dtype='int32')\n",
    "input_embedding = Embedding(n_words, n_embed_dims)(input_sentence)\n",
    "rnn_layer = SimpleRNN(2*max_len)(input_embedding)\n",
    "output = Dense(max_len)(rnn_layer)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.abs(y_true - y_pred))\n",
    "\n",
    "model = Model(inputs=[input_sentence], outputs=[output])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0), \n",
    "            loss='mae')\n",
    "model.load_weights('model-simple.h5')\n",
    "\n",
    "model.fit(dataX, dataY, epochs=2, verbose=3)\n",
    "embeddings = model.layers[1].get_weights()\n",
    "\n",
    "# print out the embedding vector associated with each word\n",
    "for i in range(2):\n",
    "    print('{}: {}'.format(idx2word[i], embeddings[0][i]))\n",
    "    \n",
    "print \"Evaluation of the Model\"\n",
    "print model.evaluate(dataX, dataY)\n",
    "print \"Saving the Weights\"\n",
    "model.save_weights('model-simple.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Training For The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/78 [===========>..................] - ETA: 0s  22.2330277761\n",
      "saving the weights\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataX, dataY, epochs=10000, batch_size=78/2, verbose=0)\n",
    "print \"  {}\".format(model.evaluate(dataX, dataY))\n",
    "print \"saving the weights\"\n",
    "model.save_weights('model-simple.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the Model\n",
    "This is how you export the model into a json file in order to be imported later. Then you export the model's weights. Later on in other experiments, you could effectively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model-simple.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model-simple.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Model\n",
    "This is how you import the model from a json file and the weights so that you don't need to train it every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model-simple.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model-simple.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model with Custom Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred Values: [[  8.00635223e+01   7.46468735e+01   5.68069305e+01   5.83343811e+01\n",
      "    3.00547719e+00   1.45626739e-02   4.47865576e-03   8.82884860e-03\n",
      "   -1.67539306e-02]]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-9dc905788026>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mprint\u001b[0m \u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# TODO - Add a check for if the word is in word2idx\n",
    "# put custom words here\n",
    "sent = ['i', 'love', 'you', 'too']\n",
    "custom_in = [0] * max_len\n",
    "for i in range(len(sent)):\n",
    "    custom_in[i] = word2idx[sent[i]]\n",
    "pred = model.predict(np.array(custom_in).reshape(1, max_len) )\n",
    "print \"Pred Values: {}\".format(pred)\n",
    "p = scaler.inverse_transform(pred)\n",
    "print '\\n\\n'\n",
    "for i in range(9):\n",
    "    if(round(p[0][i]) != 0):\n",
    "        print idx2word[ int(round(p[0][i]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
